#!/bin/bash

# Copyright (C) 2007-2014 University of Oxford
# Authors: Dave Flitney, Stephen Smith, Matthew Webster and Duncan Mortimer

# Edited: Matt Allbright
# https://github.com/mattallbright
# allbrigh@psychiatry.arizona.edu

#   Part of FSL - FMRIB's Software Library
#   http://www.fmrib.ox.ac.uk/fsl
#   fsl@fmrib.ox.ac.uk
#
#   Developed at FMRIB (Oxford Centre for Functional Magnetic Resonance
#   Imaging of the Brain), Department of Clinical Neurology, Oxford
#   University, Oxford, UK
#
#   Modified at the SCAN Lab (Social, Cognitive, and Affective Neuroscience
#   Laboratory), Department of Psychiatry, University of Arizona, Tucson,
#   Arizona
#
#   LICENCE
#
#   FMRIB Software Library, Release 5.0 (c) 2012, The University of
#   Oxford (the "Software")
#
#   The Software remains the property of the University of Oxford ("the
#   University").
#
#   The Software is distributed "AS IS" under this Licence solely for
#   non-commercial use in the hope that it will be useful, but in order
#   that the University as a charitable foundation protects its assets for
#   the benefit of its educational and research purposes, the University
#   makes clear that no condition is made or to be implied, nor is any
#   warranty given or to be implied, as to the accuracy of the Software,
#   or that it will be suitable for any particular purpose or for use
#   under any specific conditions. Furthermore, the University disclaims
#   all responsibility for the use which is made of the Software. It
#   further disclaims any liability for the outcomes arising from using
#   the Software.
#
#   The Licensee agrees to indemnify the University and hold the
#   University harmless from and against any and all claims, damages and
#   liabilities asserted by third parties (including claims for
#   negligence) which arise directly or indirectly from the use of the
#   Software or the sale of any products based on the Software.
#
#   No part of the Software may be reproduced, modified, transmitted or
#   transferred in any form or by any means, electronic or mechanical,
#   without the express permission of the University. The permission of
#   the University is not required if the said reproduction, modification,
#   transmission or transference is done without financial return, the
#   conditions of this Licence are imposed upon the receiver of the
#   product, and all original and amended source code is included in any
#   transmitted product. You may be held legally responsible for any
#   copyright infringement that is caused or encouraged by your failure to
#   abide by these terms and conditions.
#
#   You are not permitted under this Licence to use this Software
#   commercially. Use for which any financial return is received shall be
#   defined as commercial use, and includes (1) integration of all or part
#   of the source code or the Software into a product for sale or license
#   by or on behalf of Licensee to third parties or (2) use of the
#   Software or any derivative of it for research with the final aim of
#   developing software products for sale or license to a third party or
#   (3) use of the Software or any derivative of it for research with the
#   final aim of developing non-software products for sale or license to a
#   third party, or (4) use of the Software to provide any service to an
#   external organisation for which payment is received. If you are
#   interested in using the Software commercially, please contact Isis
#   Innovation Limited ("Isis"), the technology transfer company of the
#   University, to negotiate a licence. Contact details are:
#   innovation@isis.ox.ac.uk quoting reference DE/9564.
export LC_ALL=C

###########################################################################
# Edit this file in order to setup FSL to use your local compute
# cluster.
###########################################################################


###########################################################################
# The following section determines what to do when fsl_sub is called
# by an FSL program. If FSLPARALLEL is set it will attempt to pass the
# commands onto the cluster, otherwise it will run the commands
# itself. There are two values for the METHOD variable, "SGE" and
# "NONE". Note that a user can unset SGE_ROOT if they don't want the
# cluster to be used.
###########################################################################

METHOD=SLURM
unset module
if [ "x$FSLPARALLEL" = "x" ] ; then
  METHOD=NONE
else
  SINFO=`which sinfo`
  if [ "x$SINFO" = "x" ]; then
    METHOD=NONE
    echo "Warning: FSLPARALLEL environment variable is set but Slurm software not found, will run locally" >&2
  fi
fi

# stop submitted scripts from submitting jobs themselves
if [ "X$FSLSUBALREADYRUN" = "Xtrue" ] ; then
    METHOD=NONE
    echo "Warning: job on queue attempted to submit parallel jobs - running jobs serially instead" >&2
fi

if [ "X$METHOD" = "XNONE" ]; then
  SINFO=echo
fi
FSLSUBALREADYRUN=true
export FSLSUBALREADYRUN

###########################################################################
# The following auto-decides what cluster queue to use. The calling
# FSL program will probably use the -T option when calling fsl_sub,
# which tells fsl_sub how long (in minutes) the process is expected to
# take (in the case of the -t option, how long each line in the
# supplied file is expected to take). You need to setup the following
# list to map ranges of timings into your cluster queues - it doesn't
# matter how many you setup, that's up to you.
###########################################################################

map_qname ()
{
  if [[ $1 -le 20 ]] ; then
    queue=veryshort.q
  elif [[ $1 -le 120 ]] ; then
    queue=short.q
  elif [[ $1 -le 1440 ]] ; then
    queue=long.q
  else
    queue=verylong.q
  fi
  timeEst=$1
  queueCmd="--partition=$queue"
  #echo "Estimated time was $1 mins: queue name is $queue"
}

###########################################################################
# Don't change the following (but keep scrolling down!)
###########################################################################

OLD_POSIXLY_CORRECT=${POSIXLY_CORRECT}
POSIXLY_CORRECT=1
export POSIXLY_CORRECT
command=`basename $0`

usage ()
{
  cat <<EOF

$command V1.1 (SCANLab V0.2) - wrapper for job control system such as Slurm

Usage: $command [options] <command>

$command gzip *.img *.hdr
$command -q short.q gzip *.img *.hdr
$command -a x86_64 regscript rawdata outputdir ...

  -T <minutes>          Estimated job length in minutes, used to auto-set queue name.
  -q <queuename>        Possible values for <queuename> are "verylong.q", "long.q"
                        and "short.q". See below for details.
                        Default is "long.q".
  -a <arch-name>        Architecture [e.g., darwin or lx24-amd64] consumable.
                        resources from Slurm's gref.conf.
  -p <job-priority>     Lower priority [0:-1024] default = 0.
  -M <email-address>    Who to email, default = `whoami`@fmrib.ox.ac.uk.
  -j <jid>              Place a hold on this task until job jid has completed.
  -t <filename>         Specify a task file of commands to execute in parallel.
                        Tasks are similar to sbatch scripts, but lack basic options.
  -N <jobname>          Specify jobname as it will appear on queue.
  -l <logdirname>       Where to output log files.
  -m <mailoptions>      Change the cluster mail options, see qsub sbatch for
                        details.
  -z <output>           If <output> image or file already exists, do nothing and exit.
  -s <pename>,<threads> Submit a multi-threaded task - requires a
                        Parallel Environment (<pename>) to be configured for the
                        requested queues.
                        <threads> specifies the number of threads to run
  -F                    Use flags embedded in scripts to set Slurm queuing options.
  -R                    Use inputs for applications through Slurm srun command.
  -v                    Verbose mode.

  This is a modified version of FSL's fsl_sub that is compatible with the
  Slurm workload manager. The above command line options work with SGE, but
  they may not all work with Slurm. Testing is required.

Queues:

There are several batch queues configured on the cluster, each with defined CPU
time limits.

veryshort.q:This queue is for jobs which last under 30mins.
short.q:    This queue is for jobs which last up to 4h.
long.q:     This queue is for jobs which last less than 24h. Jobs run with a
            nice value of 10.
verylong.q: This queue is for jobs which will take longer than 24h CPU time.
            There is one slot per node, and jobs on this queue have a nice value
            of 15.
gpu.q:      This queue is like the verylong.q but utilizes CUDA-based GPU
            processing.

EOF

  exit 1
}

nargs=$#
if [ $nargs -eq 0 ] ; then
  usage
fi

set -- `getopt T:q:a:p:M:j:t:z:N:Fvm:R:l:s: $*`
result=$?
if [ $result != 0 ] ; then
  echo "What? Your arguments make no sense!"
fi

if [ $nargs -eq 0 ] || [ $result != 0 ] ; then
  usage
fi

POSIXLY_CORRECT=${OLD_POSIXLY_CORRECT}
export POSIXLY_CORRECT


###########################################################################
# If you have a Parallel Environment configured for OpenMP tasks then
# the variable omp_pe should be set to the name you have defined for that
# PE. The script will work out which queues have that PE setup on them.
# Note, we support openmp tasks even when Slurm is not in use.
###########################################################################

omp_pe='openmp'

###########################################################################
# The following sets up the default queue name, which you may want to
# change. It also sets up the basic emailing control.
###########################################################################

queue=long.q
queueCmd="--partition=long.q"
JobName="--job-name=fsl_sub"
scriptmode=0
peName=""
peThreads=""


###########################################################################
# In the following, you might want to change the behaviour of some
# flags so that they prepare the right arguments for the actual
# cluster queue submission program, in our case "qsub".
#
# -a sets is the cluster submission flag for controlling the required
# hardware architecture (normally not set by the calling program)
#
# -p set the priority of the job - ignore this if your cluster
# environment doesn't have priority control in this way.
#
# -j tells the cluster not to start this job until cluster job ID $jid
# has completed. You will need this feature.
#
# -t will pass on to the cluster software the name of a text file
# containing a set of commands to run in parallel; one command per
# line.
#
# -N option determines what the command will be called when you list
# running processes.
#
# -l tells the cluster what to call the standard output and standard
# -error logfiles for the submitted program.
###########################################################################

if [ -z $FSLSUBVERBOSE ] ; then
    verbose=0
else
    verbose=$FSLSUBVERBOSE;
    echo "METHOD=$METHOD : args=$@" >&2
fi

while [[ $1 != -- ]] ; do
  case $1 in
    -z)
      if [ -e $2 -o `${FSLDIR}/bin/imtest $2` = 1 ] ; then
        exit 0
      fi
      shift;;
    -T)
      map_qname $2
      shift;;
    -q)
      queue=$2
      queueCmd="--partition=$queue"
      sinfo --hide --partition=$FSLGECUDAQ >/dev/null 2>&1
      if [ $? -eq 1 ]; then
        echo "Invalid queue specified!"
        exit 127
      fi
      shift;;
    -a)
      acceptable_arch=no
      available_archs=`scontrol show nodes -o | awk '{for(i=1;i<=NF;i++){if($i~/^Arch/){print $i}}}' | cut -d '=' -f 2 | uniq`
      for a in $available_archs; do
        if [ $2 = $a ] ; then
          acceptable_arch=yes
        fi
      done
      if [[ $acceptable_arch == "yes" ]]; then
        slurm_arch="--constraint=$2"
      else
        echo "Sorry arch of $2 is not supported on this Slurm configuration!"
        echo "Should be one of:" $available_archs
        exit 127
      fi
      shift;;
    -p)
      slurm_priority="--priority=$2"
      shift;;
    -j)
      jid=$2
      slurm_hold="--dependency=afterok:$jid"
      shift;;
    -t)
      taskfile=$2
      if [ -f "$taskfile" ] ; then
        tasks=`cat $taskfile | wc -l `
      if [ $tasks -gt 1 ]; then
        slurm_tasks="--array=1-$tasks"
      elif [ $tasks -eq 0 ]; then
          echo "Task file ${taskfile} is empty"
          echo "Should be a text file listing all the commands to run!"
          exit -1
        fi
      else
          echo "Task file (${taskfile}) does not exist"
          exit -1
      fi
      shift;;
    -N)
      JobName="--job-name=$2";
      shift;;
    -M)
      mailto="--mail-user=$2";
      shift;;
    -m)
      if [ "x$mailto" != "x" ]; then
        mailTypes="--mail-type=$2";
      else
        echo "You failed to input a mail account!"
        exit -1
      fi
      shift;;
    -l)
      logDir=$2;
      logOpts="--output=$logDir/%j.out";
      shift;;
    -F)
      scriptmode=1;
      ;;
    -v)
      verbose=1
      ;;
    -R)
      scriptmode=2;
      shift;;
    -s)
      pe_string=$2;
      peName=`echo $pe_string | cut -d',' -f 1`
      peThreads=`echo $pe_string | cut -d',' -f 2`
      shift;;
  esac
  shift  # next flag
done
shift

###########################################################################
# Don't change the following (but keep scrolling down!)
###########################################################################

if [ -z "$taskfile" ] && [ -z "$1" ]; then
  echo "Either supply a command to run or a parallel task file"
  exit -1
fi

if [ -z "$taskfile" ] && [ ! -x "$1" ]; then
  which $1 >/dev/null 2>&1
  if [ $? -ne 0 ]; then
    echo "The command you have requested cannot be found or is not executable"
    exit -1
  fi
fi

if [ "x$JobName" = x ] ; then
  if [ "x$taskfile" != x ] ; then
    JobName=`basename $taskfile`
  else
    JobName=`basename $1`
  fi
fi

if [ "x$tasks" != "x" ] && [ "x$@" != "x" ] ; then
  echo "Spurious input after parsing command line: \"$@\"!"
  echo "You appear to have specified both a task file and a command to run"
  exit -1
fi

# Sets up array-specific log files
if [ "x$logOpts" != "x" ] && [ "x$tasks" != "x" ]; then
  logOpts="--output=$logDir/%A_%a.out"
fi

# Sets up default logs
if [ "x$logOpts" = "x" ]; then
  logOpts="--output=/data/var/jobs/%j.out"
  if [ "x$tasks" != "x" ]; then
    logOpts="--output=/data/var/jobs/%A_%a.out"
  fi
fi

# Sets up mail types
if [ "x$mailto" != "x" ] && [ "x$mailTypes" = "x" ]; then
  mailTypes="--mail-type=BEGIN,END,FAIL"
  MailOpts="$mailto $mailTypes"
fi

#Count node details by creating array of all non-down nodes with unique identifiers
nodeNames=($(scontrol show nodes -o | grep -v State=DOWN* | awk '{for(i=1;i<=NF;i++){if($i~/^NodeName/){print $i}}}' | cut -d '=' -f 2 | uniq))
for name in ${nodeNames[*]}; do
  totalThreads=`scontrol show node $name -o | awk '{for(i=1;i<=NF;i++){if($i~/^CPUTot/){print $i}}}' | cut -d '=' -f 2`
  #Set first node as baseline for most/least cores available per node (unused)
  if [ "$name" = ${nodeNames[0]} ]; then
    mostThreads=$nodeThreads
    mostThreadsName=${name}
    leastThreads=$nodeThreads
    leastThreadsName=${name}
  fi
  #Determine most and least cores in cluster (unused)
  if [[ $nodeThreads -gt $mostThreads ]]; then
    mostThreads=$nodeThreads
    mostThreadsName=${name}
  fi
  if [[ $nodeThreads -lt $leastThreads ]]; then
    leastThreads=$nodeThreads
    leastThreadsName=${name}
    nodesWithLeastThreads=0
  fi
  #Determine number of nodes with least cores (unused)
  if [[ $nodeThreads -eq $leastThreads ]]; then
    nodesWithLeastThreads=$(( $nodesWithLeastThreads + 1 ))
  fi
done

# Addition Parallel Environment triggers and clauses
if [ "x$peName" != "x" ]; then
  # If threads are higher than most cores, set to most cores
  if [ $peThreads -gt $totalThreads ]; then
    peThreads=$totalThreads
  fi

  # Tasks determination
  if [ -n $taskfile ] && [ "x$peThreads" != "x" ]; then
    if [ $peThreads -gt $tasks ]; then
    peThreads=$tasks
    fi
  fi

  # Let Slurm figure out the number of nodes for MPI
  if [[ $peName = "mpi" ]]; then
    # Can possibly tie leastThreads systems above into this for future project
    mpiNodes=$(( $peThreads / $leastThreads + 1))
    if [ $mpiNodes -gt $nodesWithLeastThreads ]; then
      mpiNodes = 6
    fi
    peNodes="--nodes=$mpiNodes"
    # Force Slurm to use one node for OpenMP
  elif [[ $peName = "openmp" ]]; then
    OMP_NUM_THREADS=$peThreads
    export OMP_NUM_THREADS
    peNodes="--nodes=1"

    # Use for other systems (untested)
  else
    echo "$peName is not a valid PE."
    exit -1
  fi

  # Set parallel environment options
  peCmd="$peNodes --ntasks=$peThreads"
fi

if [ $queue = "gpu.q" ];then
  queueCmd="--partition=gpu.q --gres=gpu:1"
fi
case $METHOD in

###########################################################################
# The following is the main call to the cluster, using the "sbatch" Slurm
# program. If $tasks has not been set then sbatch is running a single
# command, otherwise sbatch is processing a text file of parallel
# commands.
###########################################################################

    SLURM)
       ###########################################################################
       # Test Parallel environment options
       ###########################################################################

       # Non-tasks statement (means single srun/sbatch job)
       if [ "x$tasks" = "x" ] ; then
         if [ $scriptmode -eq 0 ] ; then
           slurm_command="sbatch --get-user-env $queueCmd $pe_option $MailOpts $JobName $logOpts $slurm_arch $slurm_hold $slurm_priority --wrap=\"$@\""
           # non-used options:
           # all used currently
         elif [ $scriptmode -eq 2 ] ; then
           # srun for testing with -R option
           slurm_command="srun $queueCmd $pe_option $MailOpts $JobName $logOpts $slurm_arch $slurm_hold $slurm_priority $@"
         else
           slurm_command="sbatch --get-user-env $queueCmd $pe_option $MailOpts $JobName $logOpts $slurm_arch $slurm_hold $slurm_priority $@"
         fi
         # Verbose clause
         if [ $verbose -eq 1 ] ; then
           echo slurm_command: $slurm_command >&2
           echo executing: $@ >&2
           echo scriptmode: $scriptmode >&2
         fi
         exec $slurm_command $@
         sleep 2
       # Tasks statement (means multiple srun/sbatch jobs)
       else
         if [ $scriptmode -eq 2 ] ; then
           # srun for testing with -R option
           slurm_command="srun $queueCmd $pe_option $MailOpts $JobName $logOpts $slurm_arch $slurm_hold $slurm_priority $slurm_tasks"
         else
           slurm_command="sbatch --get-user-env $queueCmd $pe_option $MailOpts $JobName $logOpts $slurm_arch $slurm_hold $slurm_priority $slurm_tasks"
         fi
         # Verbose clause for tasks system
         if [ $verbose -eq 1 ] ; then
           echo slurm_command: $slurm_command >&2
           #echo executing: $@ >&2
           echo control file: $taskfile >&2
           echo scriptmode: $scriptmode >&2
           commandex=`cat "$taskfile" | head -$SLURM_ARRAY_TASK_ID | tail -1`
           echo task example: $commandex >&2
         fi
         # Here bash will pipe in everything after EOF to slurm_command, executing tasks one by one
         exec $slurm_command <<EOF
#!/bin/bash
command=\`cat "$taskfile" | head -\$SLURM_ARRAY_TASK_ID | tail -1\`
exec /bin/bash -c "\$command"
EOF
        # Chill for a bit
        sleep 2
         fi
        # Opens terminal that watches output of file.
        x-terminal-emulator -e 'tail -F "$logOpts"'/$%j.out
  ;;

###########################################################################
# Don't change the following - this runs the commands directly if a
# cluster is not being used.
###########################################################################

    NONE)
    if [ "x$tasks" = "x" ] ; then
      if [ $verbose -eq 1 ] ; then
        echo executing: $@ >&2
      fi

      /bin/sh <<EOF1 > ${LogDir}${JobName}.o$$ 2> ${LogDir}${JobName}.e$$
$@
EOF1
      ERR=$?
      if [ $ERR -ne 0 ] ; then
        cat ${LogDir}${JobName}.e$$ >&2
        exit $ERR
      fi
    else
      if [ $verbose -eq 1 ] ; then
        echo "Running commands in: $taskfile" >&2
      fi

      n=1
      while [ $n -le $tasks ] ; do
        line=`sed -n -e ''${n}'p' $taskfile`
        if [ $verbose -eq 1 ] ; then
          echo executing: $line >&2
        fi
        /bin/sh <<EOF2 > ${LogDir}${JobName}.o$$.$n 2> ${LogDir}${JobName}.e$$.$n
$line
EOF2
        n=`expr $n + 1`
      done
    fi
    echo $$
    ;;

esac

###########################################################################
# Done.
###########################################################################
